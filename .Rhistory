install.packages("tm", lib="http://cran.r-project.org/web/packages/tm/");
install.packages("cluster", lib="http://cran.r-project.org/web/packages/cluster/index.html");
libs<- c("tm", "plyr", "cluster");
lapply(libs, require, character.only= TRUE);
corpus  <-Corpus(DirSource("Data/Ebola.uniq.txt"), readerControl = list(blank.lines.skip=TRUE));
file <-file("Data\Ebola.uniq.txt",open="r");
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
corpus  <-Corpus(tweets, readerControl = list(blank.lines.skip=TRUE));
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language="english")
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
#or compute cosine distance among documents
dissimilarity(tdm, method = "cosine")
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
corpus
view(corpus)
tf_idf <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
tweets
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language="english")
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument, language="english")
#creating term matrix with TF-IDF weighting
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument, language="english")
#creating term matrix with TF-IDF weighting
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
libs<- c("tm", "plyr", "cluster");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument, language="english")
#creating term matrix with TF-IDF weighting
tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
tf_idf
tf_idf[1]
tf_idf$i
tf_idf2 <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
table(tf_idf)
tf_idf$i
tf_idf$dimnames
inspect(tf_idf)
matrix <-inspect(tf_idf)
View(matrix)
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
dtm <- removeSparseTerms(dtm, sparse=0.95)
wss <-(nrow(dtm)-1)*sum(apply(dtm, 2, var))
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
# remove sparse terms < 5%
dtm <- removeSparseTerms(dtm, sparse=0.95)
# estimate the number of clusters
wss <-(nrow(dtm)-1)*sum(apply(dtm, 2, var))
tdm<-as.TermDocumentMatrix(dtm)
doc_f<-as.data.frame(inspect(tdm))
doc_f<-as.data.frame(inspect(dtm))
View(doc_f)
doc_dis<-dist(doc_f,method="euclidean")
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
# remove sparse terms < 1%
dtm <- removeSparseTerms(dtm, sparse=0.99)
tdm<-as.TermDocumentMatrix(dtm)
doc_f<-as.data.frame(inspect(tdm))
#distance_matrix
doc_dis<-dist(doc_f,method="euclidean")
View(doc_f)
dtm_clu <-kmeans(x=dtm, centers=20, iter.max=40,nstart=10)
dtm_clu$size
View(doc_f)
doc_f<-as.data.frame(inspect(tdm))
doc_dis<-dist(doc_f,method="euclidean")
doc_dis
doc_f<-as.data.frame(inspect(dtm))
doc_dis<-dist(doc_f,method="euclidean")
View(doc_f)
View(doc_f)
dissimilarity(tdm, method = "cosine")
proxy::dist(tdm, method = "cosine")
daisy(dtm, metric = "euclidean")
daisy(inspect(dtm), metric = "euclidean")
doc_d<-daisy(inspect(dtm), metric = "euclidean")
doc_d<-as.matrix(doc_d)
View(doc_d)
doc_f<-as.data.frame(inspect(dtm))
doc_dis<-dist(doc_f,method="euclidean")
doc_dis
as.matrix(doc_dif)
as.matrix(doc_dis)
doc_dis<-as.matrix(dist(doc_f,method="euclidean"))
View(doc_dis)
dtm_clu <-kmeans(x=dtm, centers=dtm[1,], iter.max=40,nstart=10)
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
dtm2 <- removeSparseTerms(dtm, sparse=0.95)
tdm<-as.TermDocumentMatrix(dtm2)
term_dm<-as.data.frame(inspect(tdm))
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
# remove sparse terms < 5%
dtm2 <- removeSparseTerms(dtm, sparse=0.95)
tdm<-as.TermDocumentMatrix(dtm2)
term_m<-as.matrix(inspect(tdm))
#calculate the distance_matrix
term_dm<-as.matrix(dist(scale(term_m),method="euclidean"))
term_dm<-dist(scale(term_m),method="euclidean")
hclust(term_dm, method="ward")
fit = hclust(term_dm, method="ward")
fit = hclust(term_dm, method="ward.D2")
plot(fit)
fit = hclust(term_dm, method="ward.D")
fit = hclust(term_dm, method="ward.D")
plot(fit)
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removeWords, c(stopwords("smart"), "amp"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x), stopwords = TRUE))
# remove sparse terms < 1%
dtm2 <- removeSparseTerms(dtm, sparse=0.98)
tdm<-as.TermDocumentMatrix(dtm2)
term_m<-as.matrix(inspect(tdm))
#calculate the distance_matrix
term_dm<-dist(scale(term_m),method="euclidean")
fit = hclust(term_dm, method="ward.D")
plot(fit)
libs<- c("tm", "plyr", "cluster", "SnowballC");
lapply(libs, require, character.only= TRUE);
#read txt articles fro'
file <-file("/Users/pengli/Desktop/workplace/WebDataMining/Data/Ebola.uniq.txt",open="r");
tweets <- readLines(file);
close(file)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, removeWords, c(stopwords("smart"), "amp"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
#corpus <- tm_map(corpus, stemDocument)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x), stopwords = TRUE))
# remove sparse terms < 1%
dtm2 <- removeSparseTerms(dtm, sparse=0.98)
tdm<-as.TermDocumentMatrix(dtm2)
term_m<-as.matrix(inspect(tdm))
#calculate the distance_matrix
term_dm<-dist(scale(term_m),method="euclidean")
fit = hclust(term_dm, method="ward.D")
plot(fit)
corpus  <-Corpus(VectorSource(tweets), readerControl = list(blank.lines.skip=TRUE));
#some preprocessing
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, c(stopwords("smart"), "amp"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
#creating term matrix with TF-IDF weighting
#tf_idf <-TermDocumentMatrix(corpus,control = list(weighting = function(x)
#  weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
#
dtm <-DocumentTermMatrix(corpus,control = list(weighting = function(x)
weightTfIdf(x), stopwords = TRUE))
# remove sparse terms < 1%
dtm2 <- removeSparseTerms(dtm, sparse=0.98)
tdm<-as.TermDocumentMatrix(dtm2)
term_m<-as.matrix(inspect(tdm))
#calculate the distance_matrix
term_dm<-dist(scale(term_m),method="euclidean")
fit = hclust(term_dm, method="ward.D")
plot(fit)
